"""
ë§¤ì¼ê²½ì œ(MK)ì—ì„œ 'ì˜¤ëŠ˜ì˜ ìš´ì„¸' ê²Œì‹œê¸€ì„ ì°¾ì•„
Google Chat Incoming Webhookìœ¼ë¡œ ì „ì†¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ê°œì¸ìš©).

íŠ¹ì´ì‚¬í•­:
- ì£¼ë§ ìš´ì„¸ëŠ” í† /ì¼ 2ì¼ì¹˜ê°€ í•œ ê²Œì‹œê¸€ë¡œ ì˜¬ë¼ì˜¬ ìˆ˜ ìˆìŒ (ì œëª©ì— ë‚ ì§œê°€ 2ê°œ).
- ê²Œì‹œê¸€ ë³¸ë¬¸ì€ í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ì´ë¯¸ì§€ 2ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ëŠ” ê²½ìš°ê°€ ìˆìŒ.

ì„¤ì¹˜: pip install requests beautifulsoup4
í™˜ê²½ë³€ìˆ˜: GCHAT_WEBHOOK  (Google Chatì—ì„œ ë°œê¸‰ë°›ì€ ì›¹í›… URL)
"""
import os
import re
import time
import logging
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urljoin
from typing import Optional, List

import requests
from bs4 import BeautifulSoup
import urllib.robotparser as robotparser

# ---------- ì„¤ì • ----------
MK_BASE = "https://www.mk.co.kr"
SEARCH_URL = "https://www.mk.co.kr/search?word=%EC%98%A4%EB%8A%98%EC%9D%98%20%EC%9A%B4%EC%84%B8"
# askjiyun (ì§€ìœ¤ì² í•™ì›) ì˜¤ëŠ˜ì˜ ìš´ì„¸ ëª©ë¡
BASE = "https://askjiyun.com"
ASKJIYUN_TODAY_LIST_URL = urljoin(BASE, "/today")
# ì œëª© ì˜ˆì‹œ:
# - ì˜¤ëŠ˜ì˜ ìš´ì„¸ 2025ë…„ 12ì›” 15ì¼ æœˆ(ìŒë ¥ 10ì›” 26ì¼)
# - ì˜¤ëŠ˜ì˜ ìš´ì„¸ 2025ë…„ 12ì›” 13ì¼ åœŸ(ìŒë ¥ 10ì›” 24ì¼)Â·2025ë…„ 12ì›” 14ì¼ æ—¥(ìŒë ¥ 10ì›” 25ì¼)
TITLE_PREFIX = "ì˜¤ëŠ˜ì˜ ìš´ì„¸"
GCHAT_WEBHOOK = os.getenv("GCHAT_WEBHOOK")

# ì „ì†¡ ìµœëŒ€ ê¸¸ì´: ë„ˆë¬´ ê¸¸ë©´ ì›¹í›…/ì±„ë„ì—ì„œ ë¬¸ì œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ìë¦„
MAX_MESSAGE_LEN = 14000
MAX_LIST_PAGES = 6  # ìµœëŒ€ ëª‡ í˜ì´ì§€ê¹Œì§€ ëª©ë¡ì„ íƒìƒ‰í• ì§€ (1-based)

# HTTP í—¤ë” (ê°„ë‹¨í•œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼)
MK_HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/127.0.0.0 Safari/537.36"),
    "Accept-Language": "ko,en;q=0.8",
    "Referer": MK_BASE,
}

ASKJIYUN_HEADERS = {
    # ModSecurity(406) íšŒí”¼ë¥¼ ìœ„í•´ ë¸Œë¼ìš°ì € í—¤ë”ë¥¼ ì¡°ê¸ˆ ë” í‰ë‚´ë‚¸ë‹¤.
    "User-Agent": ("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/127.0.0.0 Safari/537.36"),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko,en;q=0.8",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Referer": BASE,
}

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


# ---------- ìœ í‹¸: robots ì²´í¬ ----------
def allowed_by_robots(url, base_url, user_agent="*"):
    try:
        rp = robotparser.RobotFileParser()
        robots_url = urljoin(base_url, "/robots.txt")
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(user_agent, url)
    except Exception as e:
        logging.warning("robots.txt ì²´í¬ ì‹¤íŒ¨: %s (ê³„ì† ì§„í–‰)", e)
        # robots ì²´í¬ ì‹¤íŒ¨ ì‹œì—ë„ ê°œì¸ìš©ìœ¼ë¡œ ê³„ì† ì§„í–‰í•˜ê² ë‹¤ë©´ True ë°˜í™˜
        return True


# ---------- HTTP ìš”ì²­ with retry ----------
def http_get(url, *, headers=None, timeout=15, retry=3, backoff=1.2):
    last_exc = None
    for i in range(1, retry + 1):
        try:
            resp = requests.get(url, headers=headers or MK_HEADERS, timeout=timeout)
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            last_exc = e
            logging.warning("GET ì‹¤íŒ¨ (%s) %s (ì‹œë„ %d/%d)", url, e, i, retry)
            time.sleep(backoff)
    raise RuntimeError(f"GET ì‹¤íŒ¨: {url} / {last_exc}")


# ---------- ëª©ë¡ì—ì„œ ì˜¤ëŠ˜ ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° ----------
def _mk_search_page_url(page: int) -> str:
    if page <= 1:
        return SEARCH_URL
    sep = "&" if "?" in SEARCH_URL else "?"
    return f"{SEARCH_URL}{sep}page={page}"


def _clean_title_text(text: str) -> str:
    return (text or "").replace("\u00a0", " ").strip()


def find_today_post_url():
    """ê²€ìƒ‰ ê²°ê³¼(ì—¬ëŸ¬ í˜ì´ì§€)ë¥¼ ìˆœíšŒí•˜ë©° ì˜¤ëŠ˜ ë‚ ì§œê°€ í¬í•¨ëœ 'ì˜¤ëŠ˜ì˜ ìš´ì„¸' ê²Œì‹œê¸€ URLì„ ì°¾ìŠµë‹ˆë‹¤.

    ì£¼ë§ ìš´ì„¸ì²˜ëŸ¼ ë‚ ì§œê°€ 2ê°œì¸ ì œëª©ë„ 'ì˜¤ëŠ˜ ë‚ ì§œ' ë¬¸ìì—´ì´ í¬í•¨ë˜ë©´ ë§¤ì¹­ë©ë‹ˆë‹¤.
    """
    now = datetime.now(ZoneInfo("Asia/Seoul"))
    today_token = f"{now.year}ë…„ {now.month}ì›” {now.day}ì¼"
    want = re.compile(rf"{re.escape(TITLE_PREFIX)}\s+.*{re.escape(today_token)}")

    fallback_links: list[str] = []
    for page in range(1, MAX_LIST_PAGES + 1):
        url = _mk_search_page_url(page)
        logging.debug("fetching search page %d: %s", page, url)
        try:
            html = http_get(url, headers=MK_HEADERS)
        except Exception as e:
            logging.warning("ê²€ìƒ‰ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (page %d): %s", page, e)
            continue

        soup = BeautifulSoup(html, "html.parser")
        anchors = soup.find_all("a")
        for a in anchors:
            title = _clean_title_text(a.get_text(" ", strip=True))
            if TITLE_PREFIX not in title:
                continue
            href = a.get("href")
            if not href:
                continue
            post_url = urljoin(MK_BASE, href)
            fallback_links.append(post_url)
            if want.search(title):
                return post_url

    if fallback_links:
        return fallback_links[0]
    raise RuntimeError("ê²€ìƒ‰ ê²°ê³¼ì—ì„œ 'ì˜¤ëŠ˜ì˜ ìš´ì„¸' ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def find_askjiyun_today_post_url():
    """askjiyun.com /today ëª©ë¡ì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œì˜ 'ì˜¤ëŠ˜ì˜ ìš´ì„¸' ê²Œì‹œê¸€ URLì„ ì°¾ìŠµë‹ˆë‹¤."""
    now = datetime.now(ZoneInfo("Asia/Seoul"))
    today_token = f"{now.month}ì›” {now.day}ì¼"
    want = re.compile(rf"^{re.escape(TITLE_PREFIX)}\s*,\s*{re.escape(today_token)}\s*$")

    html = http_get(ASKJIYUN_TODAY_LIST_URL, headers=ASKJIYUN_HEADERS)
    soup = BeautifulSoup(html, "html.parser")

    candidates: list[tuple[str, str]] = []
    for a in soup.find_all("a"):
        title = _clean_title_text(a.get_text(" ", strip=True))
        href = a.get("href")
        if not href or "document_srl=" not in href:
            continue
        if TITLE_PREFIX not in title:
            continue
        post_url = urljoin(BASE, href)
        candidates.append((title, post_url))
        if want.search(title):
            return post_url

    # í´ë°±: ëª©ë¡ì—ì„œ ê°€ì¥ ìµœì‹  'ì˜¤ëŠ˜ì˜ ìš´ì„¸' ë§í¬ë¥¼ ì‚¬ìš©
    if candidates:
        return candidates[0][1]
    raise RuntimeError("askjiyun.com ëª©ë¡ì—ì„œ 'ì˜¤ëŠ˜ì˜ ìš´ì„¸' ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def extract_mk_images(html: str, base_url: str) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")
    candidates = []

    container_selectors = [
        "article",
        ".news_detail",
        ".article_body",
        ".news_cnt_detail_wrap",
        ".view_contents",
        "#container",
        "body",
    ]
    container = None
    for s in container_selectors:
        el = soup.select_one(s)
        if el:
            container = el
            break
    if not container:
        container = soup

    for img in container.find_all("img"):
        src = img.get("src") or img.get("data-src") or img.get("data-original")
        if not src:
            continue
        if src.startswith("data:"):
            continue
        abs_url = urljoin(base_url, src)
        if not re.search(r"\.(png|jpe?g|webp)(\?|$)", abs_url, re.I):
            continue
        if any(token in abs_url.lower() for token in ["logo", "icon", "sprite", "blank"]):
            continue
        candidates.append(abs_url)

    # ìˆœì„œ ìœ ì§€ + ì¤‘ë³µ ì œê±°
    seen = set()
    uniq = []
    for u in candidates:
        if u in seen:
            continue
        seen.add(u)
        uniq.append(u)

    # ìš´ì„¸ëŠ” ë³´í†µ 2ì¥ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë¨
    return uniq[:4]


# ---------- ê²Œì‹œê¸€ ë³¸ë¬¸ íŒŒì‹± (ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í›„ë³´ë¥¼ ë„“ê²Œ ì¡ìŒ) ----------
def parse_post(html, debug=False):
    soup = BeautifulSoup(html, "html.parser")
    # í›„ë³´ í´ë˜ìŠ¤/ì…€ë ‰í„° ì—¬ëŸ¬ê°œ ì‹œë„
    selectors = [
        ".xe_content", ".read_body", "article", ".board_read .rd_body", ".read", "#content"
    ]
    candidates = []
    for s in selectors:
        el = soup.select_one(s)
        if el:
            candidates.append(el)
            if debug:
                logging.debug("selector matched: %s -> element text length=%d", s, len(el.get_text(" ", strip=True)))

    # fallback: ë³¸ë¬¸ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ í° ìš”ì†Œë¥¼ ì„ íƒ
    if not candidates:
        # ì „ì²´ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ê°€ ë§ì€ ë¸”ë¡ì„ ê³¨ë¼ë³¸ë‹¤
        blocks = soup.find_all(['div', 'article', 'section', 'main'], limit=30)
        if blocks:
            candidates = blocks
            if debug:
                logging.debug("fallback blocks found: %d", len(blocks))

    if not candidates:
        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸
        text = soup.get_text("\n", strip=True)
        if debug:
            logging.debug("no candidates: using full document text length=%d", len(text))
        # ì¤„ ë‹¨ìœ„ë¡œ ì •ë¦¬: ê° ë¼ì¸ì„ stripí•˜ê³  ë¹ˆ ì¤„ì€ ì—°ì† 1ê°œë¡œ ì œí•œ
        lines = [ln.strip() for ln in text.splitlines()]
        cleaned = []
        prev_blank = False
        for ln in lines:
            if ln == "":
                if not prev_blank:
                    cleaned.append("")
                    prev_blank = True
                else:
                    continue
            else:
                # ë¼ì¸ ë‚´ ì—°ì† ê³µë°±ì€ í•˜ë‚˜ë¡œ ì¶•ì†Œ
                cleaned.append(re.sub(r"[ \t]{2,}", " ", ln))
                prev_blank = False
        text = "\n".join(cleaned).strip()
        return text

    # ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì„ íƒ
    body = max(candidates, key=lambda el: len(el.get_text(" ", strip=True)))
    if debug:
        logging.debug("chosen body element text length=%d", len(body.get_text(" ", strip=True)))
    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ(ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê³µìœ /ê´‘ê³  ë¸”ë¡ ë“±) ì œê±°
    for bad in body.select("script, style, noscript, iframe, header, footer, nav, form"):
        bad.decompose()
    for bad in body.select("[class*='share'], [class*='social'], [class*='ad'], [id*='share'], [id*='ad']"):
        bad.decompose()

    if debug:
        sample = body.get_text(" ", strip=True)[:500]
        logging.debug("before cleaning sample: %s", sample.replace("\n", "\\n"))

    # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ ë’¤ ë¼ì¸ ë‹¨ìœ„ë¡œ ì •ë¦¬
    text = body.get_text("\n", strip=True)
    lines = [ln.strip() for ln in text.splitlines()]
    cleaned = []
    prev_blank = False
    for ln in lines:
        if ln == "":
            if not prev_blank:
                cleaned.append("")
                prev_blank = True
            else:
                continue
        else:
            cleaned.append(re.sub(r"[ \t]{2,}", " ", ln))
            prev_blank = False
    # ì´ì œ ë¬¸ì¥/ë‹¨ë½ ë‹¨ìœ„ë¡œ ë³‘í•©: ë¹ˆ ì¤„ì€ ë‹¨ë½ êµ¬ë¶„ìœ¼ë¡œ ìœ ì§€
    paragraphs = []
    cur_lines = []
    for ln in cleaned:
        if ln == "":
            if cur_lines:
                paragraphs.append(" ".join(cur_lines).strip())
                cur_lines = []
            continue
        # ë¬¸ì¥ ë‚´ì—ì„œ ê°•ì œ ê°œí–‰(ë¬¸ì¥ ì¤‘ê°„ì— ìˆëŠ” ê²½ìš°)ë¥¼ ì œê±°í•˜ê³  ì´ì „ ë¼ì¸ê³¼ ì´ì–´ë¶™ì„
        # ë‹¨, ë¬¸ì¥ì´ ëë‚˜ëŠ” ê²½ìš°(ë§ˆì¹¨í‘œ/ë¬¼ìŒ/ê°íƒ„/%)ì—ëŠ” ê·¸ëŒ€ë¡œ ë‘ì–´ë„ ë¬´ë°©
        cur_lines.append(ln)

    if cur_lines:
        paragraphs.append(" ".join(cur_lines).strip())

    # ë¬¸ì¥ ì—°ê²° ì‹œ ì˜ëª»ëœ ê³µë°±/ë§ˆì¹¨í‘œ ë„ì–´ì“°ê¸° ì •ë¦¬
    for i, p in enumerate(paragraphs):
        # ìˆ«ìì™€ ë’¤ë”°ë¥´ëŠ” 'ë…„ìƒ' ê°™ì€ íŒ¨í„´ì˜ ì˜ëª»ëœ ë„ì–´ì“°ê¸° ë³´ì •
        p = re.sub(r"\s+([.,:;?!%])", r"\1", p)
        p = re.sub(r"\s+ë…„ìƒ", r"ë…„ìƒ", p)
        # 'ìš´ì„¸ì§€ìˆ˜\n93%.' ê°™ì´ ì˜ë ¤ ìˆë˜ ìˆ«ì ë¶™ì—¬ì“°ê¸° ë³´ì •
        p = re.sub(r"\s+(\d+)%\s*\.", r" \1%.", p)
        paragraphs[i] = p

    # ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸(ì˜ˆ: 'ì´ ê²Œì‹œë¬¼ì„ ...') ì œê±°: ì§§ì€ ë‹¨ë½ì—ë§Œ ì ìš©
    new_pars = []
    for p in paragraphs:
        if len(p) < 120 and re.search(r"ì´ ê²Œì‹œë¬¼|ê³µìœ |ëŒ“ê¸€|ì¶œì²˜", p):
            if debug:
                logging.debug("dropping short boilerplate paragraph: %r", p[:120])
            continue
        new_pars.append(p)

    text = "\n\n".join(new_pars).strip()

    # í´ë°±: ì •ì œ ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´ ì›ë¬¸(body) í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ì„ ë§‰ìŒ
    if not text:
        if debug:
            logging.debug("cleaned text empty â€” falling back to raw body/document text")
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ body ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë ¤ ì‹œë„ (ë§Œì•½ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ soup ì „ì²´ ì‚¬ìš©)
            raw_text = ''
            try:
                raw_text = body.get_text("\n", strip=True)
            except Exception:
                raw_text = soup.get_text("\n", strip=True)
            text = raw_text.strip()
            if debug:
                logging.debug("fallback raw text length=%d", len(text))
        except Exception as e:
            logging.exception("fallback to raw text failed: %s", e)
            text = ""
    if debug:
        logging.debug("final text length=%d", len(text))
        logging.debug("final sample: %s", text[:500].replace("\n", "\\n"))
    if not text:
        # ë””ë²„ê¹…ì„ ìœ„í•´ ì›ë³¸ HTMLì„ ì €ì¥
        try:
            path = os.path.join(os.getcwd(), "horoscope_debug_raw.html")
            with open(path, "w", encoding="utf-8") as f:
                f.write(html)
            logging.error("parse_post produced empty text; raw HTML saved to %s", path)
        except Exception as e:
            logging.exception("failed to save debug html: %s", e)
    return text


# ---------- Google Chat ì „ì†¡ ----------
def send_to_gchat(
    message: str,
    *,
    title: Optional[str] = None,
    link_url: Optional[str] = None,
    image_urls: Optional[List[str]] = None,
):
    if not GCHAT_WEBHOOK:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ GCHAT_WEBHOOKì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    image_urls = image_urls or []

    # ê¸°ë³¸ì€ text ë©”ì‹œì§€ë¡œ ë³´ë‚´ë˜, ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì¹´ë“œë¡œ ë³´ëƒ„.
    if image_urls:
        widgets = []
        if title or link_url:
            header_lines = []
            if title:
                header_lines.append(f"<b>{title}</b>")
            if link_url:
                header_lines.append(f"<a href=\"{link_url}\">{link_url}</a>")
            widgets.append({"textParagraph": {"text": "<br/>".join(header_lines)}})
        if message:
            widgets.append({"textParagraph": {"text": message.replace("\n", "<br/>")}})
        for u in image_urls:
            w = {"image": {"imageUrl": u}}
            if link_url:
                w["image"]["onClick"] = {"openLink": {"url": link_url}}
            widgets.append(w)
        payload = {"cards": [{"sections": [{"widgets": widgets}]}]}
    else:
        payload = {"text": message}
    # ë””ë²„ê·¸ìš©: í˜ì´ë¡œë“œë¥¼ ë¡œê¹… (ì‹¤ì œ ì „ì†¡ ì „ í™•ì¸ ê°€ëŠ¥)
    # ë©”ì‹œì§€ ë©”íŠ¸ë¦­ ë¡œê¹…: ê¸¸ì´ì™€ ê°œí–‰ ê°œìˆ˜
    nl_count = message.count("\n")
    logging.info("Sending to GChat: message length=%d chars, newlines=%d", len(message), nl_count)
    logging.debug("GChat payload JSON: %s", payload)
    try:
        r = requests.post(GCHAT_WEBHOOK, json=payload, timeout=20)
    except Exception as e:
        logging.exception("GChat POST ì‹¤íŒ¨: %s", e)
        raise
    if r.status_code >= 400:
        # ë””ë²„ê·¸ë¥¼ ìœ„í•´ ì‘ë‹µ ë³¸ë¬¸ ì¶œë ¥
        logging.error("GChat ì „ì†¡ ì‹¤íŒ¨: %d %s", r.status_code, r.text)
    r.raise_for_status()
    logging.info("Google Chat ì „ì†¡ ì„±ê³µ: %d", r.status_code)


def _strip_trailing_boilerplate(text: str) -> str:
    """ë¬¸ì„œ ëì˜ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ë¬¸êµ¬(ì˜ˆ: 'ì´ ê²Œì‹œë¬¼ì„ ...', 'ì¶œì²˜', 'ê³µìœ ' ë“±)ë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.

    ëì—ì„œë¶€í„° ì—°ì†ìœ¼ë¡œ í•´ë‹¹ íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ë¼ì¸ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not text:
        return text
    lines = text.rstrip().splitlines()
    # í•œê¸€ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì‹œì‘ íŒ¨í„´(ì—¬ëŸ¬ ì¼€ì´ìŠ¤ ë³´ìˆ˜ì ìœ¼ë¡œ í¬í•¨)
    pat = re.compile(r"^(ì´ ê²Œì‹œë¬¼|ì´ ê¸€|ì¶œì²˜|ê³µìœ |ëŒ“ê¸€)", re.I)
    removed = False
    while lines and pat.search(lines[-1].strip()):
        lines.pop()
        removed = True
    if removed:
        # ë‚¨ì€ í…ìŠ¤íŠ¸ì˜ ë ê³µë°± ì •ë¦¬
        return "\n".join(lines).rstrip()

    # ë³´ìˆ˜ì ìœ¼ë¡œ: ë§ˆì§€ë§‰ ë¬¸ì¥(í…ìŠ¤íŠ¸ ë)ì— ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ê°€ ë¶™ì–´ìˆëŠ” ê²½ìš° ì œê±°
    # ì˜ˆ: "... ì´ ê²Œì‹œë¬¼ì„ ê³µìœ í•©ë‹ˆë‹¤." ê°™ì€ í˜•íƒœ
    tail_pattern = re.compile(r"(?:\s|^)(ì´ ê²Œì‹œë¬¼ì„|ì´ ê¸€|ì¶œì²˜|ê³µìœ |ëŒ“ê¸€)[^\n]{0,200}\s*$", re.I)
    if tail_pattern.search(text):
        text = tail_pattern.sub("", text)
        return text.rstrip()

    return text


def _normalize_spacing(text: str) -> str:
    """ì›ë¬¸ì—ì„œ bs4ê°€ ë‚¨ê¸´ ë‚¨ëŠ” ê°œí–‰/ê³µë°±ì„ ë³´ìˆ˜ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

    - ê°™ì€ ë‹¨ì–´ ì‚¬ì´ì— ëŠì–´ì§„ í•œ ê¸€ì/ìˆ«ì ë¼ì¸ì€ ì•ë’¤ë¡œ ë¶™ì„
    - ë¬¸ì¥ë¶€í˜¸ ì•ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    - 'ë…„ìƒ' ê°™ì€ íŒ¨í„´ì˜ ë„ì–´ì“°ê¸° ë³´ì •
    """
    if not text:
        return text

    # 1) ë¼ì¸ë“¤ì„ ê°€ì ¸ì™€ì„œ ê° ë¼ì¸ì„ strip
    lines = [ln.strip() for ln in text.splitlines()]

    # 2) ì§§ì€ ë¼ì¸(<=3ë¬¸ì)ì€ ì´ì „ ë¼ì¸ì— ë¶™ì´ê¸° (ìˆ«ì/ë‹¨ì–´ ë¶„ë¦¬ ë°©ì§€)
    merged = []
    for ln in lines:
        if ln == "":
            merged.append("")
            continue
        if len(ln) <= 3 and merged and merged[-1] != "":
            # ê³µë°± í•„ìš” ì‹œ í•˜ë‚˜ë§Œ ì‚½ì…
            if not merged[-1].endswith(" "):
                merged[-1] = merged[-1] + " " + ln
            else:
                merged[-1] = merged[-1] + ln
        else:
            merged.append(ln)

    text2 = "\n".join(merged)

    # 3) ì—¬ëŸ¬ì¤„ë¡œ ë‚˜ë‰œ ë¬¸ì¥ ë‚´ë¶€ì˜ ê°œí–‰ì„ ê³µë°±ìœ¼ë¡œ ë°”ê¿” ë¬¸ì¥ ì—°ê²°
    #    ë‹¨ë½ êµ¬ë¶„(ë¹ˆ ì¤„)ì€ ìœ ì§€
    paragraphs = []
    cur = []
    for ln in text2.splitlines():
        if ln.strip() == "":
            if cur:
                paragraphs.append(" ".join(cur))
                cur = []
            continue
        cur.append(ln.strip())
    if cur:
        paragraphs.append(" ".join(cur))

    out = "\n\n".join(paragraphs)

    # 4) ë¬¸ì¥ë¶€í˜¸ ì• ê³µë°± ì œê±°, ë§ˆì¹¨í‘œ í›„ í•˜ë‚˜ì˜ ê³µë°± ìœ ì§€
    out = re.sub(r"\s+([.,:;?!%])", r"\1", out)
    out = re.sub(r"\.\s*([A-Za-z0-9ê°€-í£])", r". \1", out)

    # 5) 'ë…„ìƒ' ë“±ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    out = re.sub(r"(\d)\s*,\s*(\d)", r"\1, \2", out)
    out = re.sub(r"(\d)\s+ë…„ìƒ", r"\1ë…„ìƒ", out)
    out = re.sub(r"(\d+)\s*ì›”\s*(\d+)\s*ì¼", r"\1ì›” \2ì¼", out)

    # 6) ê´„í˜¸, êº½ì‡  ì£¼ë³€ ê³µë°± ì •ë¦¬
    out = re.sub(r"\s*\(\s*", " (", out)
    out = re.sub(r"\s*\)\s*", ") ", out)
    out = re.sub(r"\s*ã€ˆ\s*", " ã€ˆ", out)
    out = re.sub(r"\s*ã€‰\s*", "ã€‰ ", out)

    # ë§ˆì§€ë§‰ ê³µë°±/ì¤‘ë³µ ê³µë°± ì •ë¦¬
    out = re.sub(r"[ \t]{2,}", " ", out)
    out = re.sub(r"\n{3,}", "\n\n", out)
    return out.strip()


# ---------- main ----------
def main(argv=None):
    import argparse

    parser = argparse.ArgumentParser(description="ë§¤ì¼ê²½ì œ ì˜¤ëŠ˜ì˜ ìš´ì„¸ ì „ì†¡ê¸° (ê°œì¸ìš©)")
    parser.add_argument("--dry-run", action="store_true", help="ì›¹í›…ìœ¼ë¡œ ì „ì†¡í•˜ì§€ ì•Šê³  ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤.")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ë¡œê¹…ì„ í™œì„±í™”")
    src = parser.add_mutually_exclusive_group()
    src.add_argument("--mk-only", action="store_true", help="ë§¤ì¼ê²½ì œ ìš´ì„¸ë§Œ ì „ì†¡/ì¶œë ¥í•©ë‹ˆë‹¤.")
    src.add_argument("--jiyun-only", action="store_true", help="askjiyun.com ìš´ì„¸ë§Œ ì „ì†¡/ì¶œë ¥í•©ë‹ˆë‹¤.")
    # keep only the essential flags
    args = parser.parse_args(argv)

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    which = "both"
    if args.mk_only:
        which = "mk"
    elif args.jiyun_only:
        which = "jiyun"
    logging.info("ì‹œì‘: ì˜¤ëŠ˜ì˜ ìš´ì„¸ ì „ì†¡ (%s)", which)

    # robots ì²´í¬ (ì„ íƒ) â€” ê°œì¸ìš©ì´ë¼ë©´ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰í•˜ë„ë¡ True ë°˜í™˜
    if which in ("both", "mk") and not allowed_by_robots(SEARCH_URL, MK_BASE):
        logging.warning("robots.txtì—ì„œ í¬ë¡¤ë§ì„ ê¸ˆì§€í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ë ¤ë©´ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
    if which in ("both", "jiyun") and not allowed_by_robots(ASKJIYUN_TODAY_LIST_URL, BASE):
        logging.warning("robots.txtì—ì„œ í¬ë¡¤ë§ì„ ê¸ˆì§€í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ë ¤ë©´ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")

    jobs = []

    if which in ("both", "mk"):
        post_url = find_today_post_url()
        logging.info("MK ê²Œì‹œê¸€ URL: %s", post_url)

        html = http_get(post_url, headers=MK_HEADERS)
        soup = BeautifulSoup(html, "html.parser")

        page_title = None
        og = soup.select_one('meta[property="og:title"]')
        if og and og.get("content"):
            page_title = og.get("content").strip()
        if not page_title:
            page_title = (soup.find("title").get_text(strip=True) if soup.find("title") else TITLE_PREFIX)

        image_urls = extract_mk_images(html, post_url)
        message = "" if image_urls else f"ğŸ”® {page_title}\n{post_url}"
        jobs.append(
            {
                "title": page_title,
                "url": post_url,
                "message": message,
                "image_urls": image_urls,
            }
        )

    if which in ("both", "jiyun"):
        post_url = find_askjiyun_today_post_url()
        logging.info("askjiyun ê²Œì‹œê¸€ URL: %s", post_url)

        html = http_get(post_url, headers=ASKJIYUN_HEADERS)
        soup = BeautifulSoup(html, "html.parser")
        page_title = (soup.find("title").get_text(strip=True) if soup.find("title") else "askjiyun ì˜¤ëŠ˜ì˜ ìš´ì„¸")
        # ë³¸ë¬¸ íŒŒì‹±/ì •ë¦¬
        text = parse_post(html)
        text = _normalize_spacing(_strip_trailing_boilerplate(text))
        message = f"ğŸ”® {page_title}\n{post_url}\n\n{text}".strip()
        jobs.append(
            {
                "title": page_title,
                "url": post_url,
                "message": message,
                "image_urls": [],
            }
        )

    # ê¸¸ì´ ì œí•œ ì²˜ë¦¬(ê° ë©”ì‹œì§€ë³„)
    for j in jobs:
        if len(j["message"]) > MAX_MESSAGE_LEN:
            logging.warning("ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (%dì). ìë¦…ë‹ˆë‹¤.", len(j["message"]))
            j["message"] = j["message"][:MAX_MESSAGE_LEN] + "\n\n(ë©”ì‹œì§€ê°€ ê¸¸ì–´ ì¼ë¶€ë§Œ ì „ì†¡ë©ë‹ˆë‹¤. ì›ë¬¸ì—ì„œ ì „ì²´ í™•ì¸í•˜ì„¸ìš”.)"

    if args.dry_run:
        logging.info("Dry-run: ì›¹í›… ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤. ì¶œë ¥ìœ¼ë¡œ ëŒ€ì‹ í•©ë‹ˆë‹¤.")
        for j in jobs:
            if j["image_urls"]:
                print(f"ğŸ”® {j['title']}\n{j['url']}")
                print("\n[images]")
                for u in j["image_urls"]:
                    print(u)
                print()
            else:
                print(j["message"])
                print()
    else:
        for j in jobs:
            send_to_gchat(j["message"], title=j["title"], link_url=j["url"], image_urls=j["image_urls"])
        logging.info("ì™„ë£Œ.")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.exception("ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: %s", e)
        raise

# -*- coding: utf-8 -*-
"""
askjiyun.com/todayì˜ 'ì˜¤ëŠ˜ì˜ ìš´ì„¸, Mì›” Dì¼' ê²Œì‹œê¸€ ì „ì²´ ë³¸ë¬¸ì„
Google Chat Incoming Webhookìœ¼ë¡œ ì „ì†¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ê°œì¸ìš©).

ì„¤ì¹˜: pip install requests beautifulsoup4
í™˜ê²½ë³€ìˆ˜: GCHAT_WEBHOOK  (Google Chatì—ì„œ ë°œê¸‰ë°›ì€ ì›¹í›… URL)
"""
import os
import re
import time
import logging
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import urllib.robotparser as robotparser

# ---------- ì„¤ì • ----------
BASE = "https://askjiyun.com"
LIST_URL = urljoin(BASE, "today")
TITLE_RE = re.compile(r"^ì˜¤ëŠ˜ì˜ ìš´ì„¸,\s*\d+ì›”\s*\d+ì¼")
GCHAT_WEBHOOK = os.getenv("GCHAT_WEBHOOK")

# ì „ì†¡ ìµœëŒ€ ê¸¸ì´: ë„ˆë¬´ ê¸¸ë©´ ì›¹í›…/ì±„ë„ì—ì„œ ë¬¸ì œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ìë¦„
MAX_MESSAGE_LEN = 14000

# HTTP í—¤ë” (ê°„ë‹¨í•œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼)
UA = {
    "User-Agent": ("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/127.0.0.0 Safari/537.36"),
    "Accept-Language": "ko,en;q=0.8",
    "Referer": BASE,
}

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


# ---------- ìœ í‹¸: robots ì²´í¬ ----------
def allowed_by_robots(url, user_agent="*"):
    try:
        rp = robotparser.RobotFileParser()
        robots_url = urljoin(BASE, "/robots.txt")
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(user_agent, url)
    except Exception as e:
        logging.warning("robots.txt ì²´í¬ ì‹¤íŒ¨: %s (ê³„ì† ì§„í–‰)", e)
        # robots ì²´í¬ ì‹¤íŒ¨ ì‹œì—ë„ ê°œì¸ìš©ìœ¼ë¡œ ê³„ì† ì§„í–‰í•˜ê² ë‹¤ë©´ True ë°˜í™˜
        return True


# ---------- HTTP ìš”ì²­ with retry ----------
def http_get(url, timeout=15, retry=3, backoff=1.2):
    last_exc = None
    for i in range(1, retry + 1):
        try:
            resp = requests.get(url, headers=UA, timeout=timeout)
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            last_exc = e
            logging.warning("GET ì‹¤íŒ¨ (%s) %s (ì‹œë„ %d/%d)", url, e, i, retry)
            time.sleep(backoff)
    raise RuntimeError(f"GET ì‹¤íŒ¨: {url} / {last_exc}")


# ---------- ëª©ë¡ì—ì„œ ì˜¤ëŠ˜ ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° ----------
def find_today_post_url():
    html = http_get(LIST_URL)
    soup = BeautifulSoup(html, "html.parser")

    # 1) ì œëª© í…ìŠ¤íŠ¸ê°€ ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” a íƒœê·¸ ì°¾ê¸°
    anchors = soup.find_all("a", string=re.compile(r"^ì˜¤ëŠ˜ì˜ ìš´ì„¸,\s*\d+ì›”\s*\d+ì¼"))
    if not anchors:
        # 2) í…ìŠ¤íŠ¸ ì¡°í•©(ê³µë°±/nbsp ë“±) ë³´ì •í•´ì„œ ì°¾ê¸°
        anchors = []
        for a in soup.find_all("a"):
            txt = (a.get_text(" ", strip=True) or "").replace("\u00a0", " ")
            if re.match(r"^ì˜¤ëŠ˜ì˜ ìš´ì„¸,\s*\d+ì›”\s*\d+ì¼", txt):
                anchors.append(a)

    if not anchors:
        # 3) ëŒ€ì²´: ëª¨ë“  ë§í¬ì—ì„œ ì ‘ë‘ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ ê²€ì‚¬
        for a in soup.select("a"):
            txt = (a.get_text(" ", strip=True) or "")
            if txt.startswith("ì˜¤ëŠ˜ì˜ ìš´ì„¸,"):
                anchors.append(a)

    if not anchors:
        raise RuntimeError("ëª©ë¡ì—ì„œ 'ì˜¤ëŠ˜ì˜ ìš´ì„¸' ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ì˜¤ëŠ˜ ë‚ ì§œ ìš°ì„  íƒìƒ‰
    # use KST (Asia/Seoul) so runs on servers in UTC still select the same "ì˜¤ëŠ˜" as Korea
    now = datetime.now(ZoneInfo("Asia/Seoul"))
    md = f"{now.month}ì›” {now.day}ì¼"
    for a in anchors:
        txt = a.get_text(" ", strip=True)
        if md in txt:
            href = a.get("href")
            return urljoin(BASE, href)

    # ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼(ì²« í•­ëª©)
    href = anchors[0].get("href")
    return urljoin(BASE, href)


# ---------- ê²Œì‹œê¸€ ë³¸ë¬¸ íŒŒì‹± (ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í›„ë³´ë¥¼ ë„“ê²Œ ì¡ìŒ) ----------
def parse_post(html, debug=False):
    soup = BeautifulSoup(html, "html.parser")
    # í›„ë³´ í´ë˜ìŠ¤/ì…€ë ‰í„° ì—¬ëŸ¬ê°œ ì‹œë„
    selectors = [
        ".xe_content", ".read_body", "article", ".board_read .rd_body", ".read", "#content"
    ]
    candidates = []
    for s in selectors:
        el = soup.select_one(s)
        if el:
            candidates.append(el)
            if debug:
                logging.debug("selector matched: %s -> element text length=%d", s, len(el.get_text(" ", strip=True)))

    # fallback: ë³¸ë¬¸ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ í° ìš”ì†Œë¥¼ ì„ íƒ
    if not candidates:
        # ì „ì²´ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ê°€ ë§ì€ ë¸”ë¡ì„ ê³¨ë¼ë³¸ë‹¤
        blocks = soup.find_all(['div', 'article', 'section', 'main'], limit=30)
        if blocks:
            candidates = blocks
            if debug:
                logging.debug("fallback blocks found: %d", len(blocks))

    if not candidates:
        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸
        text = soup.get_text("\n", strip=True)
        if debug:
            logging.debug("no candidates: using full document text length=%d", len(text))
        # ì¤„ ë‹¨ìœ„ë¡œ ì •ë¦¬: ê° ë¼ì¸ì„ stripí•˜ê³  ë¹ˆ ì¤„ì€ ì—°ì† 1ê°œë¡œ ì œí•œ
        lines = [ln.strip() for ln in text.splitlines()]
        cleaned = []
        prev_blank = False
        for ln in lines:
            if ln == "":
                if not prev_blank:
                    cleaned.append("")
                    prev_blank = True
                else:
                    continue
            else:
                # ë¼ì¸ ë‚´ ì—°ì† ê³µë°±ì€ í•˜ë‚˜ë¡œ ì¶•ì†Œ
                cleaned.append(re.sub(r"[ \t]{2,}", " ", ln))
                prev_blank = False
        text = "\n".join(cleaned).strip()
        return text

    # ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì„ íƒ
    body = max(candidates, key=lambda el: len(el.get_text(" ", strip=True)))
    if debug:
        logging.debug("chosen body element text length=%d", len(body.get_text(" ", strip=True)))
    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ(ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê³µìœ /ê´‘ê³  ë¸”ë¡ ë“±) ì œê±°
    for bad in body.select("script, style, noscript, iframe, header, footer, nav, form"):
        bad.decompose()
    for bad in body.select("[class*='share'], [class*='social'], [class*='ad'], [id*='share'], [id*='ad']"):
        bad.decompose()

    if debug:
        sample = body.get_text(" ", strip=True)[:500]
        logging.debug("before cleaning sample: %s", sample.replace("\n", "\\n"))

    # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ ë’¤ ë¼ì¸ ë‹¨ìœ„ë¡œ ì •ë¦¬
    text = body.get_text("\n", strip=True)
    lines = [ln.strip() for ln in text.splitlines()]
    cleaned = []
    prev_blank = False
    for ln in lines:
        if ln == "":
            if not prev_blank:
                cleaned.append("")
                prev_blank = True
            else:
                continue
        else:
            cleaned.append(re.sub(r"[ \t]{2,}", " ", ln))
            prev_blank = False
    # ì´ì œ ë¬¸ì¥/ë‹¨ë½ ë‹¨ìœ„ë¡œ ë³‘í•©: ë¹ˆ ì¤„ì€ ë‹¨ë½ êµ¬ë¶„ìœ¼ë¡œ ìœ ì§€
    paragraphs = []
    cur_lines = []
    for ln in cleaned:
        if ln == "":
            if cur_lines:
                paragraphs.append(" ".join(cur_lines).strip())
                cur_lines = []
            continue
        # ë¬¸ì¥ ë‚´ì—ì„œ ê°•ì œ ê°œí–‰(ë¬¸ì¥ ì¤‘ê°„ì— ìˆëŠ” ê²½ìš°)ë¥¼ ì œê±°í•˜ê³  ì´ì „ ë¼ì¸ê³¼ ì´ì–´ë¶™ì„
        # ë‹¨, ë¬¸ì¥ì´ ëë‚˜ëŠ” ê²½ìš°(ë§ˆì¹¨í‘œ/ë¬¼ìŒ/ê°íƒ„/%)ì—ëŠ” ê·¸ëŒ€ë¡œ ë‘ì–´ë„ ë¬´ë°©
        cur_lines.append(ln)

    if cur_lines:
        paragraphs.append(" ".join(cur_lines).strip())

    # ë¬¸ì¥ ì—°ê²° ì‹œ ì˜ëª»ëœ ê³µë°±/ë§ˆì¹¨í‘œ ë„ì–´ì“°ê¸° ì •ë¦¬
    for i, p in enumerate(paragraphs):
        # ìˆ«ìì™€ ë’¤ë”°ë¥´ëŠ” 'ë…„ìƒ' ê°™ì€ íŒ¨í„´ì˜ ì˜ëª»ëœ ë„ì–´ì“°ê¸° ë³´ì •
        p = re.sub(r"\s+([.,:;?!%])", r"\1", p)
        p = re.sub(r"\s+ë…„ìƒ", r"ë…„ìƒ", p)
        # 'ìš´ì„¸ì§€ìˆ˜\n93%.' ê°™ì´ ì˜ë ¤ ìˆë˜ ìˆ«ì ë¶™ì—¬ì“°ê¸° ë³´ì •
        p = re.sub(r"\s+(\d+)%\s*\.", r" \1%.", p)
        paragraphs[i] = p

    # ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸(ì˜ˆ: 'ì´ ê²Œì‹œë¬¼ì„ ...') ì œê±°: ì§§ì€ ë‹¨ë½ì—ë§Œ ì ìš©
    new_pars = []
    for p in paragraphs:
        if len(p) < 120 and re.search(r"ì´ ê²Œì‹œë¬¼|ê³µìœ |ëŒ“ê¸€|ì¶œì²˜", p):
            if debug:
                logging.debug("dropping short boilerplate paragraph: %r", p[:120])
            continue
        new_pars.append(p)

    text = "\n\n".join(new_pars).strip()

    # í´ë°±: ì •ì œ ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´ ì›ë¬¸(body) í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ì„ ë§‰ìŒ
    if not text:
        if debug:
            logging.debug("cleaned text empty â€” falling back to raw body/document text")
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ body ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë ¤ ì‹œë„ (ë§Œì•½ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ soup ì „ì²´ ì‚¬ìš©)
            raw_text = ''
            try:
                raw_text = body.get_text("\n", strip=True)
            except Exception:
                raw_text = soup.get_text("\n", strip=True)
            text = raw_text.strip()
            if debug:
                logging.debug("fallback raw text length=%d", len(text))
        except Exception as e:
            logging.exception("fallback to raw text failed: %s", e)
            text = ""
    if debug:
        logging.debug("final text length=%d", len(text))
        logging.debug("final sample: %s", text[:500].replace("\n", "\\n"))
    if not text:
        # ë””ë²„ê¹…ì„ ìœ„í•´ ì›ë³¸ HTMLì„ ì €ì¥
        try:
            path = os.path.join(os.getcwd(), "horoscope_debug_raw.html")
            with open(path, "w", encoding="utf-8") as f:
                f.write(html)
            logging.error("parse_post produced empty text; raw HTML saved to %s", path)
        except Exception as e:
            logging.exception("failed to save debug html: %s", e)
    return text


# ---------- Google Chat ì „ì†¡ ----------
def send_to_gchat(message, use_card=False):
    if not GCHAT_WEBHOOK:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ GCHAT_WEBHOOKì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    if use_card:
        # Google Chat card payload keeps text formatting in a textParagraph
        payload = {"cards": [{"sections": [{"widgets": [{"textParagraph": {"text": message}}]}]}]}
    else:
        payload = {"text": message}
    # ë””ë²„ê·¸ìš©: í˜ì´ë¡œë“œë¥¼ ë¡œê¹… (ì‹¤ì œ ì „ì†¡ ì „ í™•ì¸ ê°€ëŠ¥)
    # ë©”ì‹œì§€ ë©”íŠ¸ë¦­ ë¡œê¹…: ê¸¸ì´ì™€ ê°œí–‰ ê°œìˆ˜
    nl_count = message.count("\n")
    logging.info("Sending to GChat: message length=%d chars, newlines=%d", len(message), nl_count)
    logging.debug("GChat payload JSON: %s", payload)
    try:
        r = requests.post(GCHAT_WEBHOOK, json=payload, timeout=20)
    except Exception as e:
        logging.exception("GChat POST ì‹¤íŒ¨: %s", e)
        raise
    if r.status_code >= 400:
        # ë””ë²„ê·¸ë¥¼ ìœ„í•´ ì‘ë‹µ ë³¸ë¬¸ ì¶œë ¥
        logging.error("GChat ì „ì†¡ ì‹¤íŒ¨: %d %s", r.status_code, r.text)
    r.raise_for_status()
    logging.info("Google Chat ì „ì†¡ ì„±ê³µ: %d", r.status_code)


def _strip_trailing_boilerplate(text: str) -> str:
    """ë¬¸ì„œ ëì˜ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ë¬¸êµ¬(ì˜ˆ: 'ì´ ê²Œì‹œë¬¼ì„ ...', 'ì¶œì²˜', 'ê³µìœ ' ë“±)ë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.

    ëì—ì„œë¶€í„° ì—°ì†ìœ¼ë¡œ í•´ë‹¹ íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ë¼ì¸ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not text:
        return text
    lines = text.rstrip().splitlines()
    # í•œê¸€ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì‹œì‘ íŒ¨í„´(ì—¬ëŸ¬ ì¼€ì´ìŠ¤ ë³´ìˆ˜ì ìœ¼ë¡œ í¬í•¨)
    pat = re.compile(r"^(ì´ ê²Œì‹œë¬¼|ì´ ê¸€|ì¶œì²˜|ê³µìœ |ëŒ“ê¸€)", re.I)
    removed = False
    while lines and pat.search(lines[-1].strip()):
        lines.pop()
        removed = True
    if removed:
        # ë‚¨ì€ í…ìŠ¤íŠ¸ì˜ ë ê³µë°± ì •ë¦¬
        return "\n".join(lines).rstrip()

    # ë³´ìˆ˜ì ìœ¼ë¡œ: ë§ˆì§€ë§‰ ë¬¸ì¥(í…ìŠ¤íŠ¸ ë)ì— ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ê°€ ë¶™ì–´ìˆëŠ” ê²½ìš° ì œê±°
    # ì˜ˆ: "... ì´ ê²Œì‹œë¬¼ì„ ê³µìœ í•©ë‹ˆë‹¤." ê°™ì€ í˜•íƒœ
    tail_pattern = re.compile(r"(?:\s|^)(ì´ ê²Œì‹œë¬¼ì„|ì´ ê¸€|ì¶œì²˜|ê³µìœ |ëŒ“ê¸€)[^\n]{0,200}\s*$", re.I)
    if tail_pattern.search(text):
        text = tail_pattern.sub("", text)
        return text.rstrip()

    return text


def _normalize_spacing(text: str) -> str:
    """ì›ë¬¸ì—ì„œ bs4ê°€ ë‚¨ê¸´ ë‚¨ëŠ” ê°œí–‰/ê³µë°±ì„ ë³´ìˆ˜ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

    - ê°™ì€ ë‹¨ì–´ ì‚¬ì´ì— ëŠì–´ì§„ í•œ ê¸€ì/ìˆ«ì ë¼ì¸ì€ ì•ë’¤ë¡œ ë¶™ì„
    - ë¬¸ì¥ë¶€í˜¸ ì•ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    - 'ë…„ìƒ' ê°™ì€ íŒ¨í„´ì˜ ë„ì–´ì“°ê¸° ë³´ì •
    """
    if not text:
        return text

    # 1) ë¼ì¸ë“¤ì„ ê°€ì ¸ì™€ì„œ ê° ë¼ì¸ì„ strip
    lines = [ln.strip() for ln in text.splitlines()]

    # 2) ì§§ì€ ë¼ì¸(<=3ë¬¸ì)ì€ ì´ì „ ë¼ì¸ì— ë¶™ì´ê¸° (ìˆ«ì/ë‹¨ì–´ ë¶„ë¦¬ ë°©ì§€)
    merged = []
    for ln in lines:
        if ln == "":
            merged.append("")
            continue
        if len(ln) <= 3 and merged and merged[-1] != "":
            # ê³µë°± í•„ìš” ì‹œ í•˜ë‚˜ë§Œ ì‚½ì…
            if not merged[-1].endswith(" "):
                merged[-1] = merged[-1] + " " + ln
            else:
                merged[-1] = merged[-1] + ln
        else:
            merged.append(ln)

    text2 = "\n".join(merged)

    # 3) ì—¬ëŸ¬ì¤„ë¡œ ë‚˜ë‰œ ë¬¸ì¥ ë‚´ë¶€ì˜ ê°œí–‰ì„ ê³µë°±ìœ¼ë¡œ ë°”ê¿” ë¬¸ì¥ ì—°ê²°
    #    ë‹¨ë½ êµ¬ë¶„(ë¹ˆ ì¤„)ì€ ìœ ì§€
    paragraphs = []
    cur = []
    for ln in text2.splitlines():
        if ln.strip() == "":
            if cur:
                paragraphs.append(" ".join(cur))
                cur = []
            continue
        cur.append(ln.strip())
    if cur:
        paragraphs.append(" ".join(cur))

    out = "\n\n".join(paragraphs)

    # 4) ë¬¸ì¥ë¶€í˜¸ ì• ê³µë°± ì œê±°, ë§ˆì¹¨í‘œ í›„ í•˜ë‚˜ì˜ ê³µë°± ìœ ì§€
    out = re.sub(r"\s+([.,:;?!%])", r"\1", out)
    out = re.sub(r"\.\s*([A-Za-z0-9ê°€-í£])", r". \1", out)

    # 5) 'ë…„ìƒ' ë“±ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    out = re.sub(r"(\d)\s*,\s*(\d)", r"\1, \2", out)
    out = re.sub(r"(\d)\s+ë…„ìƒ", r"\1ë…„ìƒ", out)

    # 6) ê´„í˜¸, êº½ì‡  ì£¼ë³€ ê³µë°± ì •ë¦¬
    out = re.sub(r"\s*\(\s*", " (", out)
    out = re.sub(r"\s*\)\s*", ") ", out)
    out = re.sub(r"\s*ã€ˆ\s*", " ã€ˆ", out)
    out = re.sub(r"\s*ã€‰\s*", "ã€‰ ", out)

    # ë§ˆì§€ë§‰ ê³µë°±/ì¤‘ë³µ ê³µë°± ì •ë¦¬
    out = re.sub(r"[ \t]{2,}", " ", out)
    out = re.sub(r"\n{3,}", "\n\n", out)
    return out.strip()


# ---------- main ----------
def main(argv=None):
    import argparse

    parser = argparse.ArgumentParser(description="askjiyun ì˜¤ëŠ˜ì˜ ìš´ì„¸ ì „ì†¡ê¸° (ê°œì¸ìš©)")
    parser.add_argument("--dry-run", action="store_true", help="ì›¹í›…ìœ¼ë¡œ ì „ì†¡í•˜ì§€ ì•Šê³  ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤.")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ë¡œê¹…ì„ í™œì„±í™”")
    # keep only the essential flags
    args = parser.parse_args(argv)

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    logging.info("ì‹œì‘: askjiyun ì˜¤ëŠ˜ì˜ ìš´ì„¸ ì „ì†¡ (ì „ì²´ë³¸ë¬¸)")

    # robots ì²´í¬ (ì„ íƒ) â€” ê°œì¸ìš©ì´ë¼ë©´ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰í•˜ë„ë¡ True ë°˜í™˜
    if not allowed_by_robots(LIST_URL):
        logging.warning("robots.txtì—ì„œ í¬ë¡¤ë§ì„ ê¸ˆì§€í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ë ¤ë©´ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
        # ê³„ì† ì§„í–‰í•˜ë ¤ë©´ ì£¼ì„ì²˜ë¦¬í•˜ê±°ë‚˜ í—ˆìš©ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.
        # return

    post_url = find_today_post_url()
    logging.info("ì°¾ì€ ê²Œì‹œê¸€ URL: %s", post_url)

    # ê²Œì‹œê¸€ ìš”ì²­/íŒŒì‹±
    html = http_get(post_url)
    # ê¸°ë³¸ ë™ì‘: ì •ì œëœ ë³¸ë¬¸(parse_post)ì„ ì‚¬ìš©
    text = parse_post(html, debug=args.debug)
    if args.debug:
        logging.debug("default(clean) mode: cleaned text length=%d", len(text))

    # ìµœì†Œ ë³€í™˜ ìš”êµ¬ì‚¬í•­ë§Œ ì ìš©:
    # 1) '[' ì´ì „ ë¬¸ì ì œê±° (ë³¸ë¬¸ ì‹œì‘ì—ì„œ ëŒ€ê´„í˜¸ ì „ê¹Œì§€ ì œê±°)
    # 2) '<' '>' ë° 'ã€ˆ' 'ã€‰' ì „í›„ë¡œ ì¤„ë°”ê¿ˆ 2íšŒ
    # 3) 'ë…„ìƒ' ë’¤ì— ì¤„ë°”ê¿ˆ
    # 4) '.' ë’¤ì— ì¤„ë°”ê¿ˆ
    # (ê·¸ ì™¸ ê¸°ì¡´ì˜ ì¶”ê°€ ì •ì œ ë™ì‘ì€ í•˜ì§€ ì•ŠìŒ)

    # 1) '[' ì´ì „ ë¬¸ì ì œê±°: ì²« '['ê°€ ë“±ì¥í•˜ê¸° ì „ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì œê±°
    try:
        idx = text.find('[')
        if idx != -1:
            text = text[idx:]
    except Exception:
        pass

    # ì œëª©(í˜ì´ì§€ <title>ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    try:
        soup = BeautifulSoup(html, "html.parser")
        title_raw = (soup.find("title").get_text(strip=True) if soup.find("title") else "")
        # ì‚¬ì´íŠ¸ ë„ë©”ì¸ ë“± ë¶ˆí•„ìš”í•œ ì ‘ë¯¸ì‚¬/ì ‘ë‘ì‚¬ ì œê±°
        netloc = urlparse(BASE).netloc
        page_title = title_raw
        if netloc:
            # remove occurrences of the domain surrounded by common separators
            page_title = re.sub(rf"\s*[-â€“â€”|Â·:]?\s*{re.escape(netloc)}\s*$", "", page_title)
            page_title = re.sub(rf"^{re.escape(netloc)}\s*[-â€“â€”|Â·:]?\s*", "", page_title)
            page_title = page_title.replace(netloc, "")
        # strip leftover separators/extra whitespace
        page_title = re.sub(r"^[\s\-â€“â€”|Â·:]+|[\s\-â€“â€”|Â·:]+$", "", page_title).strip()
    except Exception:
        page_title = "ì˜¤ëŠ˜ì˜ ìš´ì„¸"

    header = f"ğŸ”® *{page_title}*\n\n"
    message = header + text

    # ê¸¸ì´ ì œí•œ ì²˜ë¦¬
    if len(message) > MAX_MESSAGE_LEN:
        logging.warning("ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (%dì). ìë¦…ë‹ˆë‹¤.", len(message))
        message = message[:MAX_MESSAGE_LEN] + "\n\n(ë©”ì‹œì§€ê°€ ê¸¸ì–´ ì¼ë¶€ë§Œ ì „ì†¡ë©ë‹ˆë‹¤. ì›ë¬¸ì—ì„œ ì „ì²´ í™•ì¸í•˜ì„¸ìš”.)"

    # ì´ì œ ìš”ì²­ëœ ìµœì†Œ í›„ì²˜ë¦¬ë§Œ ìˆ˜í–‰
    # 2) ë¸Œë˜í‚· ì „í›„ë¡œ ë¹ˆ ì¤„(ë‘ ì¤„) ì‚½ì…: ASCII < > ì™€ fullwidth ã€ˆ ã€‰ ì²˜ë¦¬
    message = re.sub(r"\s*(<|ã€ˆ)", r"\n\n\1", message)
    message = re.sub(r"(>|ã€‰)\s*", r"\1\n\n", message)
    # ì—°ì† ê°œí–‰ì€ ìµœëŒ€ 2ê°œë¡œ ì œí•œ
    message = re.sub(r"\n{3,}", "\n\n", message)

    # 3) 'ë…„ìƒ' ë’¤ ì¤„ë°”ê¿ˆ
    message = re.sub(r"(ë…„ìƒ)\s*", r"\1\n", message)

    # 4) ë§ˆì¹¨í‘œ ë’¤ ì¤„ë°”ê¿ˆ
    message = re.sub(r"\.\s*", ".\n", message)

    # ë§ˆì§€ë§‰: ë³´ìˆ˜ì ìœ¼ë¡œ ë³¸ë¬¸ ëì˜ ì”ì—¬ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸(ì˜ˆ: 'ì´ ê²Œì‹œë¬¼ì„.') ì œê±°
    # ëì—ì„œë¶€í„° 3ì¤„ ì •ë„ë¥¼ ê²€ì‚¬í•˜ì—¬ 'ì´ ê²Œì‹œë¬¼' ê°™ì€ íŒ¨í„´ì´ í¬í•¨ëœ ë¼ì¸ì„ ì œê±°
    try:
        # ë¼ì¸ ë‹¨ìœ„ë¡œ ë’¤ìª½ì—ì„œë¶€í„° ê²€ì‚¬í•˜ì—¬ ë‹¤ìŒì„ ì œê±°:
        # - ì˜¨ì „íˆ '.' ë˜ëŠ” ê³µë°±ìœ¼ë¡œë§Œ ëœ ë¼ì¸ë“¤
        # - ë§ˆì§€ë§‰ ë¼ì¸ì— ë¶™ì–´ ìˆëŠ” 'ì´ ê²Œì‹œë¬¼ì„.' ë“± ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ë¬¸êµ¬
        lines = message.rstrip().splitlines()
        # 1) remove trailing lines that are only dots/spaces
        while lines and re.fullmatch(r"[.\s]+", lines[-1]):
            lines.pop()

        # 2) if the last line ends with boilerplate phrase, strip that phrase
        if lines:
            last = lines[-1]
            new_last = re.sub(r"(?:\s|^)(?:ì´ ê²Œì‹œë¬¼ì„?|ì´ ê²Œì‹œë¬¼|ì´ ê¸€|ì¶œì²˜|ê³µìœ |ëŒ“ê¸€)[\s\.:,]*$", "", last, flags=re.I).rstrip()
            lines[-1] = new_last

        # 3) if after stripping the last line becomes empty or is solely a boilerplate, pop it
        while lines and re.fullmatch(r"\s*", lines[-1]):
            lines.pop()

        message = "\n".join(lines).rstrip()
    except Exception:
        pass

    if args.dry_run:
        # dry-run: ì›¹í›… ì „ì†¡ì„ í•˜ì§€ ì•Šê³  ì¶œë ¥
        logging.info("Dry-run: ì›¹í›… ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤. ì¶œë ¥ìœ¼ë¡œ ëŒ€ì‹ í•©ë‹ˆë‹¤.")
        print(message)
    else:
        send_to_gchat(message)
        logging.info("ì™„ë£Œ.")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.exception("ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: %s", e)
        raise
